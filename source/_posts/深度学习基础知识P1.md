---
title: 深度学习基础知识P1
date: 2024-01-20 19:13:38
categories: 深度学习
tags:
  - 深度学习
  - 基础知识	
  - 卷积神经网络
---

这里主要介绍一些深度学习的基本概念

##### 卷积神经网络

一种用于通过卷积提取数据特征的神经网络

##### 卷积

通过一个卷积核在图像/矩阵上的滑动，计算出窗口元素与卷积核对应元素的乘积之和，并得到一个新的矩阵

##### padding（填充）

在输入的边界周围**填充额外的行列（通常为0）**，**防止卷积后图像快速变小**，也能够**确保边界处能够进行完整的卷积操作**

##### stride（步长）

卷积核在输入上滑动的幅度大小

上述各参数直接有这样的关系，设输入的数据尺寸为$Input\_Size$，卷积核尺寸为$Kernel\_Size$，Padding层数为$P$，步长为$S$，输出的数据尺寸为$Output\_Size$，那么有如下公式


$$
Output\_Size = \frac{(Input\_Size - Kernel\_Size + 2 \times P)}{S} + 1
$$


##### Pooling（池化）

本质是进行下采样，减小数据的尺寸，以常见的最大池化为例，选用最大的值，来代表被池化的整个区域块的值。例如2×2池化后，这四个数据选取最大的值保留，作为池化结果。

池化也可以采用步长，但是考虑到降采样的功能，一般不会像卷积一样重叠，常用的有2×2池化，步长为2

##### 感受野-Receptive Field

经过网络输出的一个点，对应原图的数据范围，也就是这个点“感受”到了原图的区域范围，如图所示。

池化（Pooling）可以快速的增加神经元对应的感受野

实际有效的感受野（Effective Receptive Field）一般比数值上的感受野要小，因为经过训练，往往会呈现中心更强，边缘更弱的分布。

![image-20260120174511538](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20260120174511538.png)



##### MLP

由**输入层、隐藏层、输出层**构成的网络，这里的隐藏层指的是除了输入&输出之外的其他中间层（隐藏层中通常使用非线性激活函数，比如**ReLU**），可以是很多层，MLP通常用来构建**分类头**，**投影头**以及**GNN的特征变换**。下面是一个图例

![image-20260120190339055](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20260120190339055.png)



##### 发展脉络

###### LeNet-5

 第一款实际应用的神经网络，奠定了基础架构——**卷积，池化，全连接**

![image-20260120162024538](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20260120162024538.png)

###### AlexNet

融入了ReLU激活函数，增加dropout减轻过拟合，并**在GPU上成功训练**

![image-20260120162059248](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20260120162059248.png)

###### VGG

提出网络深度至关重要，用**小卷积核构建更深的网络**

###### GoogleNet

在“宽度”上处理，在**同一层同时使用不同大小的卷积**核捕捉不同尺度的特征

###### ResNet

引入**残差连接**，梯度可以直接回传，防止网络退化，可以让网络更深

![image-20260120162527924](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20260120162527924.png)







